{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Знакомство с тензорами и автоматическое дифференцирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.rand(3, 2)\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print()\n",
    "\n",
    "y = torch.rand(3, 2)\n",
    "x = x + y\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.ones((3, 2), dtype=np.float32)\n",
    "z = torch.from_numpy(x)\n",
    "\n",
    "print(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3,2)\n",
    "z = x.numpy()\n",
    "\n",
    "print(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F({\\bf x})=\\left|\\left|{\\bf x}\\right|\\right|^2$$\n",
    "$$\\frac{\\partial F}{\\partial x_i} = ?$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.rand(10, requires_grad=True)\n",
    "print(x)\n",
    "print()\n",
    "\n",
    "norm = torch.dot(x, x)\n",
    "print(norm)\n",
    "print()\n",
    "\n",
    "norm.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификаторы. Сети прямого распространения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "names = [\"length\", \"width\", \"size\", \"conc\", \"conc1\", \"asym\", \"m3long\", \"m3trans\", \"alpha\", \"dist\", \"class\"]\n",
    "data = pd.read_csv('magic04.csv', names=names)\n",
    "\n",
    "x = np.asarray(data.iloc[:, :-1])\n",
    "y = np.asarray(data.iloc[:, [-1]])\n",
    "y = (y == 'g').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "X = torch.from_numpy(x.astype(np.float32))\n",
    "Y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "dataset_len = len(dataset)\n",
    "train_dataset_len = int(0.8*dataset_len)\n",
    "test_dataset_len = dataset_len - train_dataset_len\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_len, test_dataset_len])\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "INPUT_DIM  = x.shape[1]\n",
    "HIDDEN_DIM = 20\n",
    "OUTPUT_DIM = y.shape[1]\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(INPUT_DIM, HIDDEN_DIM),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(HIDDEN_DIM, OUTPUT_DIM),\n",
    "    torch.nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 25\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "for e in tqdm(range(epochs)):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        features, labels = batch\n",
    "        y_pred = model(features)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss_hist.append(loss)\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            features, labels = batch\n",
    "            y_pred = model(features)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            count += 1\n",
    "            total_loss += loss\n",
    "        test_loss_hist.append(total_loss/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = np.asarray(train_loss_hist)\n",
    "test_loss_hist = np.asarray(test_loss_hist)\n",
    "plt.plot(train_loss_hist, '-*', label='Train loss')\n",
    "plt.plot(test_loss_hist, '-*', label='Test loss')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros((2,2))\n",
    "true_and_scores = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        features, labels = batch\n",
    "        \n",
    "        scores = model(features)\n",
    "        y_pred = (scores > 0.5)\n",
    "        for t, p in zip(labels.view(-1), y_pred.view(-1)):\n",
    "                confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "        true_and_scores.append(np.hstack([labels.numpy(), scores.numpy()]))\n",
    "        \n",
    "true_and_scores = np.vstack(true_and_scores)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix.view(-1)\n",
    "accuracy  = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall    = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "baccuracy = 0.5 * (specificity + recall)\n",
    "    \n",
    "print(\"Accuracy                  = {:.4f}\".format(accuracy))\n",
    "print(\"Ballanced accuracy        = {:.4f}\".format(baccuracy))\n",
    "print(\"Precision (PPV)           = {:.4f}\".format(precision))\n",
    "print(\"Recall (sensitivity, TPR) = {:.4f}\".format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "y_true = true_and_scores[:,0]\n",
    "scores = true_and_scores[:,1]\n",
    "\n",
    "min_score, max_score = np.min(scores), np.max(scores)\n",
    "bins = np.linspace(min_score, max_score, 25)\n",
    "plt.figure()\n",
    "plt.hist(scores[y_true.reshape(-1) == 0], bins, alpha=0.5, label='Hadron (negative)')\n",
    "plt.hist(scores[y_true.reshape(-1) == 1], bins, alpha=0.5, label='Gamma (positive)')\n",
    "plt.xlabel(\"Decision function (value)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "\n",
    "fpr, tpr, _ = sklearn.metrics.roc_curve(y_true, scores)\n",
    "auc = sklearn.metrics.roc_auc_score(y_true, scores)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.title(\"Receiver operating characteristic\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "print(\"AUC                       = {:.4f}\".format(auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.1** Это задание на *бонусные балы*. Вам нужно использовать готовый код классификатора выше, чтобы улучшить качество классификатора в смысле *AUC*.\n",
    "\n",
    "Попробуйте исследовать как результат меняется в зависимости от числа скрытых нейронов, вида функции активации, применяемого для обучения оптимизатора. Помогает ли добавление дополнительного скрытого слоя? Поможет ли применение анализа главных компонент для входных данных перед обучением нейросети?\n",
    "\n",
    "*Бонусные баллы* положены за лучшие пять решений среди всей группы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.2** Перед вами набор данных рукописных цифр MNIST (черно-белые изображения, размером 28x28 пикселей каждое). Задача состоит в том, чтобы сделать автоэнкодер на базе свёрточной нейронной сети. Размерность пространства закодированного представления (латентный слой) должна быть 8. Подберите параметры свёрточных слоев.\n",
    "\n",
    "*Бонусные баллы* положены за автоэнкодер с наименьшим среднеквадратичным отклонением `torch.nn.MSELoss` на тестовых данных и размерностью закодированного представления 8. Попробуйте добавить новые свёрточные слои или менять конфигурацию существующих, или вид функций активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.manual_seed(43)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.0, 1.0)\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root=\"./mnist\", download=True, transform=transform, train=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root=\"./mnist\", download=True, transform=transform, train=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "n_col = 4\n",
    "for i in range(n_col):\n",
    "    ax = plt.subplot(3, n_col, i + 1)\n",
    "    plt.imshow(images[i].reshape(28,28), cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "\n",
    "INPUT_CH  = images.shape[1]\n",
    "HIDDEN1_CH = # ???\n",
    "HIDDEN2_CH = # ???\n",
    "LATENT_DIM = 8\n",
    "\n",
    "class CNNAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(INPUT_CH, HIDDEN1_CH, 3, stride=2),\n",
    "            torch.nn.ReLU(True),\n",
    "            \n",
    "            torch.nn.Conv2d(HIDDEN1_CH, HIDDEN2_CH, 3, stride=2, bias=False),\n",
    "            torch.nn.ReLU(True),\n",
    "            \n",
    "            torch.nn.Conv2d(HIDDEN2_CH, LATENT_DIM, 6, bias=False),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.LayerNorm((LATENT_DIM, 1, 1)),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(LATENT_DIM, HIDDEN2_CH, 6, bias=False),\n",
    "            torch.nn.ReLU(True),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(HIDDEN2_CH, HIDDEN2_CH, 3, stride=2, bias=False),\n",
    "            torch.nn.ReLU(True),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(HIDDEN2_CH, HIDDEN1_CH, 4, stride=2, bias=False),\n",
    "            torch.nn.ReLU(True),\n",
    "            \n",
    "            torch.nn.ConvTranspose2d(HIDDEN1_CH, INPUT_CH, 1, bias=False),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "model = CNNAutoEncoder()\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 5\n",
    "train_loss_hist = []\n",
    "test_loss_hist = []\n",
    "for e in range(epochs):\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        images, labels = batch\n",
    "        _, decoded = model(images)\n",
    "        loss = loss_fn(decoded, images)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    train_loss_hist.append(loss)\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        total_loss = 0\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, labels = batch\n",
    "            _, decoded = model(images)\n",
    "            loss = loss_fn(decoded, images)\n",
    "            count += 1\n",
    "            total_loss += loss\n",
    "        test_loss_hist.append(total_loss/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_hist = np.asarray(train_loss_hist)\n",
    "test_loss_hist = np.asarray(test_loss_hist)\n",
    "plt.plot(train_loss_hist, '-*', label='Train loss')\n",
    "plt.plot(test_loss_hist, '-*', label='Test loss')\n",
    "plt.legend()\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        images, labels = batch\n",
    "        _, decoded = model(images)\n",
    "        loss = loss_fn(decoded, images)\n",
    "        \n",
    "        count += 1\n",
    "        total_loss += loss\n",
    "\n",
    "    print(\"Average loss = {:.4f}\".format(total_loss/count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.3** Используя обученную модель автоэнкодера нарисуйте для четырех изображений тестового набора данных: само изображение, его закодированное представление в виде гистограммы, а так же раскодированное изображение. Ниже приведена заготовка кода рисующего изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7.5))\n",
    "n_col = 4\n",
    "for i in range(n_col):\n",
    "    ax = plt.subplot(3, n_col, i + 1)\n",
    "    inp = # ???\n",
    "    plt.imshow(inp, cmap='gray')\n",
    "    ax = plt.subplot(3, n_col, i + 1 + n_col)\n",
    "    ecnoded = # ??? \n",
    "    plt.bar(np.arange(encoded[i].shape[0]), encoded[i])\n",
    "    ax = plt.subplot(3, n_col, i + 1 + n_col * 2)\n",
    "    decoded = # ???\n",
    "    plt.imshow(decoded, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.4** Проделайте задание **3a.3**, но теперь на вход автоэнкодера следует подать изображения с добавленным к ним равномерно распределенным шумом амплитуды `noise_level`. После добавления шума, входные данные возможно придётся перенормировать, так как нейросеть ожидает, что в каждом пикселе содержится число от $0.0$ до $1.0$. Придумайте как это сделать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_level = 0.2\n",
    "\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "n_col = 4\n",
    "for i in range(n_col):\n",
    "    ax = plt.subplot(3, n_col, i + 1)\n",
    "    inp = # ???\n",
    "    plt.imshow(inp, cmap='gray')\n",
    "    ax = plt.subplot(3, n_col, i + 1 + n_col)\n",
    "    ecnoded = # ??? \n",
    "    plt.bar(np.arange(encoded[i].shape[0]), encoded[i])\n",
    "    ax = plt.subplot(3, n_col, i + 1 + n_col * 2)\n",
    "    decoded = # ???\n",
    "    plt.imshow(decoded, cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.5** В задании **3a.4** вы должны были обнаружить, что автоэнкодер можно использовать для удаления шума из данных. В векторе `noise_levels` задана сетка амплитуды шумов для входных данных. Предлагается построить график зависимости от амплитуды шума для среднеквадратичного отклонения `torch.nn.MSELoss` между выходом автоэнкодера и незашумленным (истинным) входным изображением. Используйте полный тестовый датасет для вычисления значений. При шуме нулевой амплитуды (нет шума) среднеквадратичное отклонение будет определяться самой моделью. При постепенном добавлении шума он будет мешать нейросети распознать исходное изображение абсолютно корректно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_levels = np.linspace(0.0, 0.5, 10)\n",
    "noise = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            images, labels = batch\n",
    "            noised = (images + torch.randn(images.shape) * noise_level) / (1.0 + noise_level)\n",
    "            _, decoded = model(noised)\n",
    "            loss = loss_fn(decoded, images)\n",
    "        \n",
    "            count += 1\n",
    "            total_loss += loss\n",
    "        noise.append([total_loss/count])\n",
    "        \n",
    "plt.plot(noise_levels, np.asarray(noise), \"-*\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.xlabel(\"Noise level\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.6** Аналогично заданию **3a.3** сгенерируйте и подайте на вход декодера случайные данные, совпадающие по своим статистическим свойствам с содержимым латентного слоя. Нарисуйте сгенерированные декодером изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7.5))\n",
    "n_col = 4\n",
    "for i in range(n_col):\n",
    "    ax = plt.subplot(3, n_col, i + 1)\n",
    "    encoded = # ???\n",
    "    plt.bar(np.arange(encoded[i].shape[0]), encoded[i])\n",
    "    ax = plt.subplot(3, n_col, i + 1 + n_col\n",
    "    decoded = # ???\n",
    "    plt.imshow(, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 3a.7** Перед вами набор данных изображений STL10. Он состоит из цветных фотографий размером 96x96, отнесённых к одному из десяти различных классов: аэроплан, птичка, автомобиль, котик, олень, собака, лошадь, обезьяна, корабль, грузовик. Задача состоит в том, чтобы построить классификатор используя свёрточные нейронные сети, используя \"transfer learning\". Библиотека `torchvision` содержит в себе уже обученную глубокую свёрточную нейросеть AlexNet (`torchvision.models.alexnet`), эта сеть позволяет классифицировать фотографии на 1000 различных классов (набор данных ImageNet) и состоит из двух частей:\n",
    " * нескольких свёрточных слоев в начале сети\n",
    " * нескольких полносвязных слоев в конце сети\n",
    " \n",
    "Можно считать, что первая половина сети генерирует некоторый набор универсальных признаков, на основе которого можно классифицировать фотографии из других наборов данных. Вам предлагается заменить часть нейросети состоящей из полносвязных слоев для решения задачи классификации STL10. Свёрточные слои обучать не следует, т.е. их веса остаются фиксированными и известными.\n",
    "\n",
    "В качестве функции потерь для классификатора убодно использовать `torch.nn.CrossEntropyLoss`.\n",
    "\n",
    "Используйте готовую функцию `sklearn.metrics.confusion_matrix` и класс `sklearn.metrics.ConfusionMatrixDisplay` чтобы вычислить и нарисовать матрицу ошибок для тестового набора данных. Используйте готовую функцию `sklearn.metrics.classification_report`, чтобы подсчитать точность (precision) и полноту (recall) для каждого из целевых классов.\n",
    "\n",
    "*Бонусные балы* полагаются за лучшее среди всей группы значение функции потерь на тестовом наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms\n",
    "\n",
    "def show(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(256),\n",
    "    torchvision.transforms.CenterCrop(224),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    normalize])\n",
    "\n",
    "train_dataset = torchvision.datasets.STL10(\"./stl10\", split='train', transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.STL10(\"./stl10\", split='test', transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "class_names = np.array([\"airplane\", \"bird\", \"car\", \"cat\", \"deer\", \"dog\", \"horse\", \"monkey\", \"ship\", \"truck\"])\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(class_names[labels.numpy()])\n",
    "show(torchvision.utils.make_grid(images, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models\n",
    "\n",
    "alexnet = torchvision.models.alexnet(pretrained=True)\n",
    "for param in alexnet.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(43)\n",
    "\n",
    "HIDDEN_DIM = # ???\n",
    "\n",
    "class ClassifierModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        \n",
    "        self.features = alexnet.features\n",
    "        self.avgpool = alexnet.avgpool\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            # тут должна была быть полносвязная сеть, но она пропала.\n",
    "            # известно, что на входе должно быть 256 * 6 * 6 значений,\n",
    "            # а на выходе вектор из 10 признаков\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = ClassifierModel()\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "        images, labels = batch\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print(\"epoch = {} batch = {} loss = {}\".format(e, i, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total_loss = 0\n",
    "true_and_pred = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        images, labels = batch\n",
    "        output = model(images)\n",
    "        loss = loss_fn(output, labels)\n",
    "        \n",
    "        count += 1\n",
    "        total_loss += loss\n",
    "        \n",
    "        y_pred = torch.max(output, axis=1).indices\n",
    "        true_and_pred.append(np.vstack([labels.numpy(), y_pred.numpy()]))\n",
    "\n",
    "true_and_pred = np.hstack(true_and_pred)\n",
    "\n",
    "print(\"Average loss = {:.4f}\".format(total_loss/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "#print(classification_report(???))\n",
    "\n",
    "cm = # confusion_matrix ???\n",
    "\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
    "_ = cmd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
